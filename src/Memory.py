import torch
from transformers import pipeline
import logging

# Set device to GPU if available
if torch.cuda.is_available():
    device:int = 0
else:
    device:int = -1

MAX_MEMORY_SIZE:int = 2000

class ChatbotMemory:
    def __init__(self, conv:list = []):
        self.conversation_history = conv

    def update_memory(self, user_input:str, bot_response:str)->None:
        """
        Updates the conversation history in the Chatbot's memory with the user input and bot response.
        Args:
            user_input (str): The input provided by the user.
            bot_response (str): The response generated by the Chatbot.
        Returns:
            None
        """
        self.conversation_history.append(f"'user': {user_input}, 'bot': {bot_response}")
        
        if memory_counter(self.conversation_history) > 1000:
            self.conversation_history = compressed_memory(self.conversation_history)
            logging.info("Memory compressed.")
        
        if len(self.conversation_history) > MAX_MEMORY_SIZE:
            self.conversation_history.pop(0)
            logging.info("Memory trimmed.")  
        return 0

    def get_memory(self):
        """
        Returns the conversation history stored in the Chatbot's memory.

        Returns:
            The conversation history.
        """
        return self.conversation_history
    
def _get_compressed_memory(sentence:str)->str:
    """
    Compresses the input sentence using the Facebook BART model for summarization.

    Args:
        sentence: The input sentence to be compressed.

    Returns:
        str: The compressed summary of the input sentence.
    """
    summarizer:str = pipeline("summarization",model="facebook/bart-large-cnn",device=device)
    summary:str = summarizer(sentence, max_length=50, min_length=5, do_sample=False)
    return summary[0]['summary_text']

def compressed_memory(conv_hist:list)->list:
    """
    Compresses each sentence in the conversation history list using summarization.

    Args:
        conv_hist: List of sentences representing the conversation history.

    Returns:
        list: List of compressed summaries for each sentence in the conversation history.
    """
    # return [_get_compressed_memory(sentence) for sentence in conv_hist]
    return [_get_compressed_memory(' '.join(conv_hist[i:i+5])) for i in range(0, len(conv_hist), 5)]
        

def memory_counter(conv_hist:list)->int:
    """
    Counts the total number of words in the conversation history list.

    Args:
        conv_hist: List of sentences representing the conversation history.

    Returns:
        int: Total number of words in the conversation history.
    """
    st = ''.join(conv_hist)
    return len(st.split())